{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6aYgnzA2I9d"
      },
      "source": [
        "## Importing the CIFAR-100 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-zZzqr062I9f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euNbEV-D2I9g",
        "outputId": "eb419315-04d4-43bc-945a-075dbae91806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 2s 0us/step\n",
            "(50000, 32, 32, 3) (50000, 1)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data(label_mode='fine')\n",
        "\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "098ezWUT2I9h",
        "outputId": "124d042f-8b0f-4967-ef13-b0c3574ecd4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dp9X6eNL2I9h"
      },
      "outputs": [],
      "source": [
        "x_train = (x_train / 255 - 0.5) * 2\n",
        "x_test = (x_test / 255 - 0.5) * 2\n",
        "\n",
        "enc = LabelBinarizer()\n",
        "\n",
        "y_train = enc.fit_transform(y_train)\n",
        "y_test = enc.fit_transform(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms4zegUs2I9i",
        "outputId": "f4198486-dac9-4128-b64e-95691f71656f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "print(x_test[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY8Dp8gn2I9i"
      },
      "source": [
        "## Inception v4 Implementation\n",
        "\n",
        "The CIFAR100 dataset is composed of 32x32 pixel images as oppopsed to the 299x299 pixel images of the Imagenet network for which the Inceptionv4 network was originally comstructed. There a significant downsizing of the network is necessary to accomodate the 10x size difference.\n",
        "\n",
        "Inception blocks are used to create feature maps, and reduction blocks are used to downsize the filter size for teh inception blocks\n",
        "Intuitions:\n",
        "1. Put 5x5 convolutions in stem to downsize image to smaller size, and expand filter space for the Inception-A blocks.\n",
        "2. First inception blocks TRANSFORM the feature map (with 1x1 convolutions) while KEEPING the same image size.\n",
        "3. Reduction-A blocks further reduce the image size and expand filter space for Inception-B blocks.\n",
        "4. Inception-B blocks further transform the feature map.\n",
        "5. Probably not neccessary, but if it is add Reduction-B blocks\n",
        "6. A final AveragePooling layer reduces  \n",
        "\n",
        "1. Drop the reduction blocks as the image is already small adn there is no need to reduce image size (probably only dimension reduction is needed)\n",
        "   1. this probably won't work as different size feature extraction is necessary\n",
        "2. Reduce the number of Conv2DBatchNorm layers in each block (Inception, Reduction, Stem) in order not to decrease the image size down to one so quickly\n",
        "3. Change all filter numbers to half."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "HXTg2ZfN2I9j",
        "outputId": "e024b31b-9c81-4fef-9a04-6c8686782ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pre-stem size: (None, 32, 32, 3)\n",
            "(None, 32, 32, 16)\n",
            "(None, 32, 32, 32)\n",
            "(None, 15, 15, 48)\n",
            "Concat 1 (None, 15, 15, 80)\n",
            "(None, 15, 15, 32)\n",
            "(None, 13, 13, 48)\n",
            "(None, 15, 15, 32)\n",
            "(None, 15, 15, 32)\n",
            "(None, 15, 15, 32)\n",
            "(None, 13, 13, 48)\n",
            "Concat 2 (None, 13, 13, 96)\n",
            "(None, 11, 11, 96)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5b4418d8d2d9>\u001b[0m in \u001b[0;36m<cell line: 182>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyInception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-5b4418d8d2d9>\u001b[0m in \u001b[0;36mMyInception\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# for _ in range(4):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-5b4418d8d2d9>\u001b[0m in \u001b[0;36mStem\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mbranch_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbranch_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post-stem size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/layers/merging/concatenate.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 )\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 11, 11, 96), (None, 6, 6, 96)]"
          ]
        }
      ],
      "source": [
        "#import tensorflow as tf\n",
        "from keras.layers import Dense, Conv2D, Input, MaxPooling2D, BatchNormalization, Activation, AveragePooling2D, Dropout, Concatenate, Flatten\n",
        "#from keras.layers import *\n",
        "from keras import Model\n",
        "\n",
        "\n",
        "# TODO: Go to Inceptionv2 paper to understand Conv2D_BN and fine tune parameters potentially\n",
        "def Conv2DBatchNorm(x, filters, kernel_size=(3, 3), strides=(1, 1), padding='same'):\n",
        "\n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(x)\n",
        "            #    kernel_regularizer=keras.regularizers.l2(0.00004),\n",
        "            #    kernel_initializer=keras.initializers.VarianceScaling(scale=0.2, mode='fan_in', distribution='normal', seed=None)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    print(x.shape)\n",
        "    return x\n",
        "\n",
        "def Stem(x):\n",
        "    print(\"pre-stem size:\", x.shape)\n",
        "\n",
        "    x = Conv2DBatchNorm(x, 16, (5, 5))\n",
        "    x = Conv2DBatchNorm(x, 32, (3, 3))\n",
        "\n",
        "    branch_0 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid')(x)\n",
        "    branch_1 = Conv2DBatchNorm(x, 48, (3, 3), strides=(2, 2), padding='valid')\n",
        "\n",
        "    x = Concatenate()([branch_0, branch_1])\n",
        "    print(\"Concat 1\", x.shape)\n",
        "    branch_0 = Conv2DBatchNorm(x, 32, (1, 1))\n",
        "    branch_0 = Conv2DBatchNorm(branch_0, 48, (3, 3), padding='valid')\n",
        "\n",
        "    branch_1 = Conv2DBatchNorm(x, 32, (1, 1))\n",
        "    #branch_1 = Conv2DBatchNorm(branch_1, 32, (1, 7))\n",
        "    #branch_1 = Conv2DBatchNorm(branch_1, 32, (7, 1))\n",
        "    \n",
        "    branch_1 = Conv2DBatchNorm(branch_1, 43, (3, 3))\n",
        "    branch_1 = Conv2DBatchNorm(branch_1, 48, (3, 3), padding='valid')\n",
        "\n",
        "    x = Concatenate()([branch_0, branch_1])\n",
        "    print(\"Concat 2\", x.shape)\n",
        "    branch_0 = Conv2DBatchNorm(x, 96, (3, 3))\n",
        "    branch_1 = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Concatenate()([branch_0, branch_1])\n",
        "\n",
        "    print(\"post-stem size:\", x.shape)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def InceptionA(x):\n",
        "    print(\"pre-A size:\", x.shape)\n",
        "\n",
        "    branch_0 = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "    branch_0 = Conv2DBatchNorm(branch_0, 96, (1, 1))\n",
        "\n",
        "    branch_1 = Conv2DBatchNorm(x, 96, (1, 1))\n",
        "\n",
        "    branch_2 = Conv2DBatchNorm(x, 64, (1, 1))\n",
        "    branch_2 = Conv2DBatchNorm(branch_2, 96, (3, 3))\n",
        "\n",
        "    branch_3 = Conv2DBatchNorm(x, 64, (1, 1))\n",
        "    branch_3 = Conv2DBatchNorm(branch_3, 96, (3, 3))\n",
        "    branch_3 = Conv2DBatchNorm(branch_3, 96, (3, 3))\n",
        "\n",
        "    x = Concatenate()([branch_0, branch_1, branch_2, branch_3])\n",
        "    print(\"post-A size:\", x.shape)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def InceptionB(x):\n",
        "    print(\"pre-B size:\", x.shape)\n",
        "\n",
        "    branch_0 = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "    branch_0 = Conv2DBatchNorm(branch_0, 128, (1, 1))\n",
        "\n",
        "    branch_1 = Conv2DBatchNorm(x, 384, (1, 1))\n",
        "\n",
        "    branch_2 = Conv2DBatchNorm(x, 192, (1, 1))\n",
        "    branch_2 = Conv2DBatchNorm(branch_2, 224, (1, 7))\n",
        "    branch_2 = Conv2DBatchNorm(branch_2, 256, (7, 1))\n",
        "\n",
        "    branch_3 = Conv2DBatchNorm(x, 192, (1, 1))\n",
        "    branch_3 = Conv2DBatchNorm(branch_3, 192, (1, 7))\n",
        "    branch_3 = Conv2DBatchNorm(branch_3, 224, (7, 1))\n",
        "    branch_3 = Conv2DBatchNorm(branch_3, 224, (1, 7))\n",
        "    branch_3 = Conv2DBatchNorm(branch_3, 256, (7, 1))\n",
        "\n",
        "    x = Concatenate()([branch_0, branch_1, branch_2, branch_3])\n",
        "    print(\"post-B size:\", x.shape)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def InceptionC(x):\n",
        "    print(\"pre-C size:\", x.shape)\n",
        "\n",
        "    branch_0 = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "    branch_0 = Conv2DBatchNorm(branch_0, 256, (1, 1))\n",
        "\n",
        "    branch_1 = Conv2DBatchNorm(x, 256, (1, 1))\n",
        "\n",
        "    branch_2 = Conv2DBatchNorm(x, 384, (1, 1))\n",
        "    branch_2_0 = Conv2DBatchNorm(branch_2, 256, (1, 3))\n",
        "    branch_2_1 = Conv2DBatchNorm(branch_2, 256, (3, 1))\n",
        "\n",
        "    branch_3 = Conv2DBatchNorm(x, 384, (1, 1))\n",
        "    branch_3 = Conv2DBatchNorm(branch_3, 448, (1, 3))\n",
        "    branch_3 = Conv2DBatchNorm(branch_3, 512, (3, 1))\n",
        "    branch_3_0 = Conv2DBatchNorm(branch_3, 256, (3, 1))\n",
        "    branch_3_1 = Conv2DBatchNorm(branch_3, 256, (1, 3))\n",
        "\n",
        "    x = Concatenate()([branch_0, branch_1, branch_2_0, branch_2_1, branch_3_0, branch_3_1])\n",
        "    print(\"post-C size:\", x.shape)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def ReductionA(x):\n",
        "    print(\"pre-RedA size:\", x.shape)\n",
        "\n",
        "    branch_0 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid')(x)\n",
        "\n",
        "    branch_1 = Conv2DBatchNorm(x, 384, (3, 3), strides=(2, 2), padding='valid')\n",
        "\n",
        "    branch_2 = Conv2DBatchNorm(x, 192, (1, 1))\n",
        "    branch_2 = Conv2DBatchNorm(branch_2, 224, (3, 3))\n",
        "    branch_2 = Conv2DBatchNorm(branch_2, 256, (3, 3), strides=(2, 2), padding='valid')\n",
        "\n",
        "    x = Concatenate()([branch_0, branch_1, branch_2])\n",
        "    print(\"post-RedA size:\", x.shape)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def ReductionB(x):\n",
        "    print(\"pre-RedB size:\", x.shape)\n",
        "\n",
        "    branch_0 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid')(x)\n",
        "\n",
        "    branch_1 = Conv2DBatchNorm(x, 192, (1, 1))\n",
        "    branch_1 = Conv2DBatchNorm(branch_1, 192, (3, 3), strides=(2, 2), padding='valid')\n",
        "\n",
        "    branch_2 = Conv2DBatchNorm(x, 256, (1, 1))\n",
        "    branch_2 = Conv2DBatchNorm(branch_2, 256, (1, 7))\n",
        "    branch_2 = Conv2DBatchNorm(branch_2, 320, (7, 1))\n",
        "    branch_2 = Conv2DBatchNorm(branch_2, 320, (3, 3), strides=(2, 2), padding='valid')\n",
        "\n",
        "    x = Concatenate()([branch_0, branch_1, branch_2])\n",
        "    print(\"post-RedB size:\", x.shape)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def MyInception():\n",
        "    inputs = Input(shape=(32, 32, 3), name='input')\n",
        "\n",
        "    x = Stem(inputs)\n",
        "\n",
        "    # for _ in range(4):\n",
        "    #     x = InceptionA(x)\n",
        "\n",
        "    # x = ReductionA(x)\n",
        "\n",
        "    # for _ in range(7):\n",
        "    #     x = InceptionB(x)\n",
        "\n",
        "    # x = ReductionB(x)\n",
        "\n",
        "    # for _ in range(3):\n",
        "    #     x = InceptionC(x)\n",
        "\n",
        "    x = AveragePooling2D()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    outputs = Dense(100, activation='softmax', name='output')(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='MyInception')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = MyInception()\n",
        "\n",
        "model.compile()\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "The Stem Reduces image size from 299 to 35 in the original Inceptionv4 architecture which is a 1/10 scale. That is too large of a downscale and we can't afford that with a 32 by 32 image. That's why my stem only reduces image 1/3. I've also reduced all teh filter numbers to half their original number, because presumably for a smaller mage a smaller repsresentation will be sufficient\n",
        "\n",
        "Questions:\n",
        "What metrics to use\n",
        "What to log\n",
        "How many blocks to use"
      ],
      "metadata": {
        "id": "WuTwD5YL54Cr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f491y2S52I9k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a5fa9285215e4b0b41af508f79a138b9553c6fbd1ae80ad44751154a42b4e6d9"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}